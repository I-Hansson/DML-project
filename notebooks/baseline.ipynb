{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d6775ad",
   "metadata": {},
   "source": [
    "## Baseline for genre model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6392755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/isachansson/DML-project/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4184e11ba60478699ab338c3b1ffcda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8980b9d7b1fd4d01b177463397b70a67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3de7268b7c74ddc92cacd48c92c9419",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "down\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc3899a0c3704fe385f575df6215aab2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25857764cbd348868a1a8a4c0b933c1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e543a6ae0b0d44a38af65dded7367853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e349724aee0741ac8004e73134265f8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs shape: torch.Size([1, 5])\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import os\n",
    "os.environ[\"HF_HUB_DISABLE_XET\"] = \"1\"\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"0\"\n",
    "os.environ[\"HF_HUB_OFFLINE\"] = \"0\"\n",
    "cache_dir = Path.home() / \".cache\" / \"huggingface\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(\n",
    "    model_name,\n",
    "    cache_dir=cache_dir,\n",
    "    use_safetensors=False,\n",
    "    resume_download=False\n",
    ")\n",
    "print('down')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "input_sequence = \"Genre: Indie\\n\\n\"\n",
    "input_ids = tokenizer.encode(input_sequence, return_tensors='pt').to(device)\n",
    "\n",
    "print(f\"Input IDs shape: {input_ids.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7050460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating lyrics...\n",
      " samples in 2.67 seconds\n",
      "\n",
      "============================================================\n",
      "BASELINE SAMPLE 1\n",
      "============================================================\n",
      "Genre: Indie\n",
      "\n",
      "\n",
      "Product Information: The original design by Mihai. This time it's a light brown with red highlights (I don't know if you can tell from the white of his eyes). He has an easy-to read, well drawn look and does not seem overly obsessed with color at all.\" - GajaricMintz & Stijn\n",
      "\n",
      "============================================================\n",
      "BASELINE SAMPLE 2\n",
      "============================================================\n",
      "Genre: Indie\n",
      "\n",
      "\n",
      "Posts : 3,959 Re-uploaded to Steam « Reply #12 on the Poster | Threads [ ] » Quote from? The only way you can remove one is with a banhammer. In other words, that's it! You're telling me what I already know for sure; they all have rules and everything else has been set by them... So let us start off saying something cool about our game.. First up we'll just ask how many times your friend or coop player made any use of this tool but i think most people would agree at least twice in their life as well since everyone does need tools like these anyways so please do not post more than once before going through my rulebook.... Second was why are there no \"Counters\" (see below) listed above if anything??? No matter which team makes an invasion charge - here isn't much detail yet because every time its clear who had invaded/demoted someone thats either doing 2nd invade(usually within 90 minutes after starting invasions). If another person starts out holding defensive counters then when asked where will he take some information regarding whether his counter should be taken back down / defended using offensive attacks?, immediately answer 1st Answer 8th Ask 4e Answers 7c Crescendo 9b Total Posts 3939 Author Message Posted By Kommandone Originally posted by But those were pretty awesome questions guys..... Forgot even being able talk directly against anyone while playing games anymore??\n",
      "\n",
      "============================================================\n",
      "BASELINE SAMPLE 3\n",
      "============================================================\n",
      "Genre: Indie\n",
      "\n",
      "\n",
      "The game is being developed by Guerrilla Games and will be released for Windows, Mac OS X, Linux and PlayStation 4. The development team has been invited to create a free demo that includes an online leaderboard of developers from all over the globe working together in order not only with Unreal Engine 3 (Oculus Rift) but also some other games based on it as well! All backers who have played through this Kickstarter campaign can find out more details about what makes such great VR demos awesome at www-projectunity.com/gameplay!\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "print(\"Generating lyrics...\")\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "with torch.inference_mode():\n",
    "    sample_outputs = model.generate(\n",
    "        input_ids,\n",
    "        do_sample=True,\n",
    "        max_length=300,\n",
    "        temperature=0.9,\n",
    "        top_k=50,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.2,\n",
    "        num_return_sequences=3,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\" samples in {elapsed:.2f} seconds\")\n",
    "\n",
    "# Decode and print\n",
    "for i, output in enumerate(sample_outputs, 1):\n",
    "    text = tokenizer.decode(output, skip_special_tokens=True)\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"BASELINE SAMPLE {i}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c4c1f4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dml-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
